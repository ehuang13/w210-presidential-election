{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmh/anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package stopwords to /home/rmh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import join, exists\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_key_words = ['job', 'economy', 'unemployment', 'race', 'election', 'president', \n",
    "                    'policy', 'immigration', 'sex', 'woman', 'health', \n",
    "                    'global', 'warming', 'police', 'black', 'rating', 'growth', 'undocumented', \n",
    "                    'russia', 'allegation', 'infrastructure', 'republican', 'democrat', 'senate', 'house',\n",
    "                    'donald', 'trump', 'hillary', 'clinton', 'barack', 'obama', 'john', 'mccain',\n",
    "                    'mitt', 'romney', 'george', 'bush', 'john', 'kerry', 'gore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_election_data(start_date, end_date, election_year):\n",
    "\n",
    "    data = []\n",
    "    dayrange = range((end_date - start_date).days + 1)\n",
    "\n",
    "    ARTICLES_DIR = join('data', 'guardian', election_year)\n",
    "\n",
    "    for daycount in dayrange:\n",
    "        dt = start_date + timedelta(days=daycount)\n",
    "        datestr = dt.strftime('%Y-%m-%d')\n",
    "        fname = join(ARTICLES_DIR, datestr + '.json')\n",
    "        with open(fname) as f:\n",
    "            for hd in json.load(f):\n",
    "                data.append(hd)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_aspects_with_fuzzy_match(aspects, news_hd):\n",
    "    asp_match = []\n",
    "    pos = []\n",
    "    lbls = []\n",
    "    hdls = []\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "\n",
    "    for hd in news_hd:\n",
    "        #skip = False\n",
    "        for w in hd.split():\n",
    "            if w not in stopWords:\n",
    "                for kw in aspects:\n",
    "                    score = fuzz.partial_ratio(kw, w)\n",
    "\n",
    "                    if score >= 80:\n",
    "                        s_idx = hd.find(w)\n",
    "                        asp_match.append(kw)\n",
    "                        pos.append(str(s_idx) + ',' + str(s_idx + len(w)))\n",
    "                        lbls.append('positive')\n",
    "                        hdls.append(hd)\n",
    "                        #skip = True\n",
    "                        #break\n",
    "\n",
    "            #if skip == True:\n",
    "            #    break\n",
    "            \n",
    "    return hdls, asp_match, lbls, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(year, hds, pos, asp, lbl):\n",
    "    with open('data/processed-data/' + year + '/headlines.txt', \"w\") as output:\n",
    "        for row in hds:\n",
    "            output.write(str(row) + '\\n')\n",
    "\n",
    "    with open('data/processed-data/' + year + '/position.txt', \"w\") as output:\n",
    "        for row in pos:\n",
    "            output.write(str(row) + '\\n')\n",
    "\n",
    "    with open('data/processed-data/' + year + '/term.txt', \"w\") as output:\n",
    "        for row in asp:\n",
    "            output.write(str(row) + '\\n')\n",
    "\n",
    "    with open('data/processed-data/' + year + '/label.txt', \"w\") as output:\n",
    "        for row in lbl:\n",
    "            output.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_trainable_data(start_date, end_date, year, fz_kw):\n",
    "    print('----' + year + '----------')\n",
    "    news_headlines = get_election_data(start_date, end_date, year)\n",
    "    headlines, aspects, labels, positions = identify_aspects_with_fuzzy_match(fz_kw, news_headlines)\n",
    "    write_data(year, headlines, positions, aspects, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----2016----------\n",
      "----2012----------\n",
      "----2008----------\n",
      "----2004----------\n",
      "----2000----------\n"
     ]
    }
   ],
   "source": [
    "gen_trainable_data(date(2016, 8, 8), date(2016, 11, 8), '2016', aspect_key_words)\n",
    "gen_trainable_data(date(2012, 8, 6), date(2012, 11, 6), '2012', aspect_key_words)\n",
    "gen_trainable_data(date(2008, 8, 4), date(2008, 11, 4), '2008', aspect_key_words)\n",
    "gen_trainable_data(date(2004, 8, 2), date(2004, 11, 2), '2004', aspect_key_words)\n",
    "gen_trainable_data(date(2000, 8, 7), date(2000, 11, 7), '2000', aspect_key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
